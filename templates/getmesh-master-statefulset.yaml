apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: {{ template "getmesh.fullname" . }}-master
  labels:
    app.kubernetes.io/name: {{ include "getmesh.name" . }}
    helm.sh/chart: {{ include "getmesh.chart" . }}
    app.kubernetes.io/instance: {{ .Release.Name }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
    app.kubernetes.io/version: {{ .Chart.AppVersion }}
    app.kubernetes.io/component: getmesh-master
spec:
  replicas: 1 # we use only 1 master and X replicas in this chart
  selector:
    matchLabels:
      app.kubernetes.io/name: {{ include "getmesh.name" . }}
      app.kubernetes.io/instance: {{ .Release.Name }}
  serviceName: "getmesh-master"
  template:
    metadata:
      labels:
        app.kubernetes.io/name: {{ include "getmesh.name" . }}
        app.kubernetes.io/instance: {{ .Release.Name }}
        app.kubernetes.io/component: getmesh-master
        app.kubernetes.io/part-of: getmesh-api
    spec:
      terminationGracePeriodSeconds: 30
      initContainers:
      - name: init
        image: "{{ .Values.toolsImage.repository }}:{{ .Values.image.tag }}"
        command: ["/bin/sh", "/config/init-master-node.sh"]
        securityContext:
          runAsUser: 0
        volumeMounts:
        - name: {{ template "getmesh.nfs.name" . }}
          mountPath: "/uploads"
          subPath: {{ template "getmesh.fullname.no.override" . }}-uploads
        - name: {{ template "getmesh.nfs.name" . }}
          mountPath: "/backups-nfs"
          subPath: {{ template "getmesh.fullname.no.override" . }}-backups
        - name: getmesh-config
          mountPath: /config
        - name: getmesh-data-master
          mountPath: "/graphdb"
          subPath: "graphdb"
        env:
        - name: "REPAIR_DATABASE_BEFORE_STARTUP"
          value: "false"
      containers:
      - name: {{ .Chart.Name }}
        image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
        imagePullPolicy: {{ .Values.image.pullPolicy }}
        command: ["/bin/sh", "/config/startup-master-node.sh"]
        env:
        {{- range $name, $value := .Values.getmesh.coreEnvironmentVariables }}
         - name: {{ tpl $name $ }}
           value: {{ tpl $value $ | quote }}
        {{- end }}
        {{- range $name, $value := .Values.getmesh.additionalEnvironmentVariables }}
         - name: {{ tpl $name $ }}
           value: {{ tpl $value $ | quote }}
        {{- end }}
        {{- if .Values.oauth2.enabled }}
         - name: MESH_AUTH_OAUTH2_ENABLED
           value: "true"
         - name: MESH_AUTH_OAUTH2_SERVER_CONF_AUTH_SERVER_URL
           value: {{ tpl .Values.oauth2.authServerUrl . | quote }}
        {{- end }}
        ports:
          - name: http
            containerPort: 8080
            protocol: TCP
          - name: monitoring
            containerPort: 8081
            protocol: TCP
          - name: obinary
            containerPort: 2424
            protocol: TCP
          - name: ohttp
            containerPort: 2480
            protocol: TCP
          - name: hazelcast
            containerPort: 5701
            protocol: TCP
        livenessProbe:
          httpGet:
            path: {{ .Values.getmesh.master.livenessProbe.httpGet.path | default "/api/v1/" }}
            port: http
          initialDelaySeconds: {{ .Values.getmesh.master.livenessProbe.initialDelaySeconds | default 30 }}
          periodSeconds: {{ .Values.getmesh.master.livenessProbe.periodSeconds | default 10 }}
          failureThreshold: {{ .Values.getmesh.master.livenessProbe.failureThreshold | default 60 }}
        #master readinessProbe is associated to hazelcast port vs replica readiness is associated to status
        readinessProbe:
          tcpSocket:
            {{- if .Values.getmesh.cluster.enabled }}
            port: hazelcast
            {{- else }}
            port: http
            {{- end }}
          initialDelaySeconds: {{ .Values.getmesh.master.readinessProbe.initialDelaySeconds | default 30 }}
          periodSeconds: {{ .Values.getmesh.master.readinessProbe.periodSeconds | default 10 }}
          failureThreshold: {{ .Values.getmesh.master.readinessProbe.failureThreshold | default 60 }}

        resources:
{{ toYaml .Values.getmesh.master.resources | indent 12 }}
        volumeMounts:
          - name: {{ template "getmesh.nfs.name" . }}
            mountPath: "/uploads"
            subPath: {{ template "getmesh.fullname.no.override" . }}-uploads
          - name: getmesh-config
            mountPath: "/config"
          - name: getmesh-data-master
            mountPath: "/elasticsearch/data"
            subPath: "elasticsearch"
          - name: getmesh-data-master
            mountPath: "/graphdb"
            subPath: "graphdb"
          - name: {{ template "getmesh.nfs.name" . }}
            mountPath: "/backups-nfs"
            subPath: {{ template "getmesh.fullname.no.override" . }}-backups
      securityContext:
        runAsUser: 1000
        fsGroup: 1000 #user to run used by getmesh official image, if not setted we got some filesystem errors at startup
      volumes:
      - name: getmesh-config
        configMap:
          name: {{ template "getmesh.fullname" . }}-config
      {{- if not .Values.persistence.enabled }}
      - name: getmesh-data-master
        emptyDir: {}
      - name: getmesh-uploads
        emptyDir: {}
      {{- else }}
      {{- if and (eq .Values.persistence.nfs.enabled true) (eq .Values.persistence.enabled true) }}
      - name: getmesh-uploads
        persistentVolumeClaim:
          claimName: {{ template "getmesh.fullname.no.override" . }}-uploads
      {{- end}}
      {{- if .Values.persistence.existingClaim }}
      - name: getmesh-data-master
        persistentVolumeClaim:
          claimName: {{ .Values.persistence.data.existingClaim }}
      {{- else }}
      {{- if .Values.getmesh.master.tolerations }}
      tolerations:
{{ toYaml .Values.getmesh.master.tolerations | indent 8 }}
      {{- end }}
      {{- if .Values.getmesh.master.nodeSelector }}
      nodeSelector:
{{ toYaml .Values.getmesh.master.nodeSelector | indent 8 }}
      {{- end }}
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                topologyKey: kubernetes.io/hostname
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: {{ include "getmesh.name" . }}
                    app.kubernetes.io/instance: {{ .Release.Name }}
  volumeClaimTemplates:
    - metadata:
        name: getmesh-data-master
        labels:
          app: {{ template "getmesh.name" . }}
          chart: "{{ .Chart.Name }}"
          release: {{ .Release.Name }}
          heritage: {{ .Release.Service }}
      spec:
        accessModes:
        {{- range .Values.persistence.accessModes }}
          - {{ . | quote }}
        {{- end }}
        resources:
          requests:
            storage: {{ .Values.persistence.size | quote }}
      {{- if .Values.persistence.storageClass }}
      {{- if (eq "-" .Values.persistence.storageClass) }}
        storageClassName: ""
      {{- else }}
        storageClassName: {{ .Values.persistence.storageClass | quote }}
      {{- end }}
      {{- end }}
    {{- end }}
  {{- end }}

